{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09de3e92-a64a-4bdb-a1d1-aa7666a2ca7e",
   "metadata": {},
   "source": [
    "# Project: Predicting Customer Churn for a Brazilian E-commerce Platform\n",
    "## Project Overview\n",
    "The goal of this project is to analyze a rich, real-world e-commerce dataset to understand the primary drivers of customer churn. Students will build a machine learning model that can predict which customers are at a high risk of \"churning\" (i.e., not making another purchase). The final deliverable is not just the model, but a report that provides actionable insights for the business to reduce churn.\n",
    "## Learning Objectives\n",
    "### SQL:\n",
    "1. Loading data into a SQL database (we'll use SQLite for simplicity).\n",
    "2. Writing complex queries with JOINs, GROUP BY, HAVING, and window functions.\n",
    "3. Performing initial Exploratory Data Analysis (EDA) directly in SQL.\n",
    "4. Creating an \"Analytics Base Table\" (ABT) by joining multiple data sources.\n",
    "### Python (with Pandas, Matplotlib, Seaborn, Scikit-learn):\n",
    "1. Connecting Python to a SQL database to execute queries and load data into DataFrames.\n",
    "2. Advanced data cleaning and manipulation.\n",
    "3. In-depth EDA and data visualization to uncover patterns.\n",
    "4. Feature Engineering: Creating new predictive features from existing data.\n",
    "5. Building and training several classification models (e.g., Logistic Regression, Random Forest, XGBoost).\n",
    "6. Evaluating model performance using appropriate metrics (Accuracy, Precision, Recall, F1-Score, ROC-AUC).\n",
    "7. Interpreting model results to extract business insights (e.g., feature importance).\n",
    "### Presentation & Business Acumen:\n",
    "1. Structuring a data science project from problem definition to solution.\n",
    "2. Communicating technical findings to a non-technical audience.\n",
    "3. Deriving actionable business recommendations from data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e71734-3ec1-4fea-b29a-086eacbd15d2",
   "metadata": {},
   "source": [
    "### The Dataset\n",
    "We will use the \"Brazilian E-Commerce Public Dataset by Olist\" available on Kaggle. It's perfect because it contains 100k orders from 2016 to 2018 and is spread across multiple relational tables, forcing the use of SQL.\n",
    "### Link to Dataset: https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce\n",
    "It includes the following (and more) tables:\n",
    "1. olist_customers_dataset.csv\n",
    "2. olist_orders_dataset.csv\n",
    "3. olist_order_items_dataset.csv\n",
    "4. olist_order_payments_dataset.csv\n",
    "5. olist_order_reviews_dataset.csv\n",
    "6. olist_products_dataset.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17faa296-df07-4b2c-bcd8-f2ed07951100",
   "metadata": {},
   "source": [
    "### Phase 1: Database Setup and Data Exploration with SQL\n",
    "### Goal: Get the data into a queryable format and perform initial analysis.\n",
    "### Setup:\n",
    "1. Download the dataset from Kaggle.\n",
    "2. Use Python's sqlite3 library to create a new database file (e.g., ecommerce.db).\n",
    "3. Use Pandas to read each CSV file and load it as a table into the SQLite database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50dd0989-b61b-4827-8ef8-2cb346dcce06",
   "metadata": {},
   "source": [
    "#### Source Code (Python - 1_setup_database.py):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "715374b4-ccb0-4183-bdb2-f0a94b803fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a411faf7-2455-49a0-b319-b46a261bd45e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mysql-connector-python in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (9.4.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install mysql-connector-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ce630cd4-4174-4ba7-92fb-92122875bb43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: SQLAlchemy in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.0.43)\n",
      "Requirement already satisfied: pandas in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.3.2)\n",
      "Requirement already satisfied: pymysql in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.1.1)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from SQLAlchemy) (3.2.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from SQLAlchemy) (4.14.1)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install SQLAlchemy pandas pymysql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "89cd999a-a715-49c7-b579-4d7c073ec2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sqlalchemy import create_engine\n",
    "\n",
    "# MySQL URI format\n",
    "# engine = create_engine(\"mysql+pymysql://root:Rp@2121994@localhost/ecommerce_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c882c80d-86f3-496f-a4cc-3871ccd4f0cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymysql in c:\\users\\admin\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pymysql\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fa7e649c-b17b-4282-af37-f397fe1c786a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MySQL connection established successfully.\n",
      "General Error: [Errno 2] No such file or directory: 'C:\\\\Users\\\\ADMIN\\\\Desktop\\\\Customer_Churn_Project\\\\Chustomer_Churn ML Project\\\\Chustomer_Churn ML Project\\\\olist_customers_dataset.csv'\n",
      "MySQL connection closed.\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "import os\n",
    "import mysql.connector\n",
    "\n",
    "# MySQL URI format\n",
    "# Raw password: Basaram7036@ → URL-encoded: Basaram7036%40\n",
    "engine = create_engine(\"mysql+pymysql://root:Basaram7036%40@localhost/ecommerce_db\")\n",
    "\n",
    "# Path to the dataset files (use raw string to avoid escape issues)\n",
    "DATA_PATH = r\"C:\\Users\\ADMIN\\Desktop\\Customer_Churn_Project\\Chustomer_Churn ML Project\\Chustomer_Churn ML Project\"\n",
    "\n",
    "# MySQL connector config\n",
    "config = {\n",
    "    'user': 'root',\n",
    "    'password': 'Basaram7036@',\n",
    "    'host': 'localhost',\n",
    "    'database': 'ecommerce_db',\n",
    "    'raise_on_warnings': True\n",
    "}\n",
    "\n",
    "# Connect and insert\n",
    "try:\n",
    "    conn = mysql.connector.connect(**config)\n",
    "    cursor = conn.cursor()\n",
    "    print(\"MySQL connection established successfully.\")\n",
    "\n",
    "    # CSV files to load\n",
    "    csv_files = [\n",
    "        'olist_customers_dataset.csv',\n",
    "        'olist_orders_dataset.csv',\n",
    "        'olist_order_items_dataset.csv',\n",
    "        'olist_order_payments_dataset.csv',\n",
    "        'olist_order_reviews_dataset.csv',\n",
    "        'olist_products_dataset.csv',\n",
    "        'olist_sellers_dataset.csv',\n",
    "        'product_category_name_translation.csv'\n",
    "    ]\n",
    "\n",
    "    # Load CSVs into MySQL\n",
    "    for file in csv_files:\n",
    "        df = pd.read_csv(os.path.join(DATA_PATH, file))\n",
    "        table_name = file.replace('.csv', '').replace('olist_', '').replace('_dataset', '')\n",
    "        df.to_sql(name=table_name, con=engine, if_exists='replace', index=False)\n",
    "        print(f\"Table '{table_name}' created successfully from '{file}'.\")\n",
    "\n",
    "except mysql.connector.Error as err:\n",
    "    print(f\"MySQL Error: {err}\")\n",
    "except Exception as e:\n",
    "    print(f\"General Error: {e}\")\n",
    "finally:\n",
    "    if conn.is_connected():\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        print(\"MySQL connection closed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64366fd5-89fb-4741-947e-8204984944e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1be6de-4f2d-4fee-9c11-250b21758db5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca16af5f-64eb-44d3-8c5a-6c40168dec59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84e802d-af0e-4480-9e35-a7395205e263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MySQL connection established successfully.\n",
      "Table 'customers' created successfully from 'olist_customers_dataset.csv'.\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "import os\n",
    "import mysql.connector\n",
    "\n",
    "# MySQL URI format\n",
    "engine = create_engine(\"mysql+pymysql://root:Basaram7036%40@localhost/ecommerce_db\")\n",
    "\n",
    "# ✅ Correct path to your dataset folder\n",
    "DATA_PATH = r\"C:\\Users\\ADMIN\\Desktop\\Customer_Churn_Project\\Chustomer_Churn ML Project\\Chustomer_Churn ML Project\\Data Sets\"\n",
    "\n",
    "# MySQL connector config\n",
    "config = {\n",
    "    'user': 'root',\n",
    "    'password': 'Basaram7036@',\n",
    "    'host': 'localhost',\n",
    "    'database': 'ecommerce_db',\n",
    "    'raise_on_warnings': True\n",
    "}\n",
    "\n",
    "# Connect and insert\n",
    "try:\n",
    "    conn = mysql.connector.connect(**config)\n",
    "    cursor = conn.cursor()\n",
    "    print(\"MySQL connection established successfully.\")\n",
    "\n",
    "    # ✅ Now includes all 9 CSV files\n",
    "    csv_files = [\n",
    "        'olist_customers_dataset.csv',\n",
    "        'olist_geolocation_dataset.csv',   # <── Added this missing file\n",
    "        'olist_orders_dataset.csv',\n",
    "        'olist_order_items_dataset.csv',\n",
    "        'olist_order_payments_dataset.csv',\n",
    "        'olist_order_reviews_dataset.csv',\n",
    "        'olist_products_dataset.csv',\n",
    "        'olist_sellers_dataset.csv',\n",
    "        'product_category_name_translation.csv'\n",
    "    ]\n",
    "\n",
    "    # Load CSVs into MySQL\n",
    "    for file in csv_files:\n",
    "        df = pd.read_csv(os.path.join(DATA_PATH, file))\n",
    "        table_name = file.replace('.csv', '').replace('olist_', '').replace('_dataset', '')\n",
    "        df.to_sql(name=table_name, con=engine, if_exists='replace', index=False)\n",
    "        print(f\"Table '{table_name}' created successfully from '{file}'.\")\n",
    "\n",
    "except mysql.connector.Error as err:\n",
    "    print(f\"MySQL Error: {err}\")\n",
    "except Exception as e:\n",
    "    print(f\"General Error: {e}\")\n",
    "finally:\n",
    "    if conn.is_connected():\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        print(\"MySQL connection closed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0146ee07-ef70-4307-94f7-377ee1da9c7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9162357b-814c-44ab-9f9a-222c75bb302c",
   "metadata": {},
   "source": [
    "## SQL Exploratory Analysis:\n",
    "1. Now, Employees can connect to this database using a GUI like DB Browser for SQLite or directly through Python.\n",
    "2. They should answer business questions using only SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64547ea-418a-4c3b-b734-a146cc8cbb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- What is the distribution of customers by state?\n",
    "# Distribution of customers by state\n",
    "query1 = \"\"\"\n",
    "SELECT customer_state, COUNT(customer_unique_id) AS customer_count\n",
    "FROM customers\n",
    "GROUP BY customer_state\n",
    "ORDER BY customer_count DESC;\n",
    "\"\"\"\n",
    "df1 = pd.read_sql(query1, engine)\n",
    "print(\"Customers by State:\\n\", df1.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc814385-3864-441f-9127-27032a9e4a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ab0322-3492-4977-b4dd-75097d794800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- What are the most common payment methods?\n",
    "# Most common payment methods\n",
    "query2 = \"\"\"\n",
    "SELECT payment_type, COUNT(*) AS transaction_count\n",
    "FROM order_payments\n",
    "GROUP BY payment_type\n",
    "ORDER BY transaction_count DESC;\n",
    "\"\"\"\n",
    "df2 = pd.read_sql(query2, engine)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fb1a35-0c09-4bbc-a54b-890f80ba161f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be31a87-83a2-412c-abd0-e41cf2729a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- What is the average review score?\n",
    "# Average review score\n",
    "query3 = \"\"\"\n",
    "SELECT AVG(review_score) AS average_review_score\n",
    "FROM order_reviews;\n",
    "\"\"\"\n",
    "df3 = pd.read_sql(query3, engine)\n",
    "print(\"\\n Average Review Score:\\n\", df3)\n",
    "\n",
    "# Close connection\n",
    "cursor.close()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a039fa2c-e73b-47ac-a176-5db2975a0cc6",
   "metadata": {},
   "source": [
    "# Phase 2: Defining Churn & Creating the Master Table (SQL + Python)\n",
    "## Goal: Define what \"churn\" means for this dataset and create a single, wide table (an Analytics Base Table) that contains all the features for each customer.\n",
    "### Defining Churn: \n",
    "Since there's no \"subscription cancelled\" column, we must create our own target variable. A common definition: \"A customer has churned if they have not made a purchase in the last 6 months.\"\n",
    "### Feature Engineering with SQL: \n",
    "The most critical step. We will write one large SQL query to create our master table. This query will calculate features for each customer_unique_id.\n",
    "\n",
    "This is a complex query, perfect for demonstrating SQL prowess."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570dda5c-4092-4c5c-b5fd-bb1a4fe6bd27",
   "metadata": {},
   "source": [
    "### Source Code (SQL - 2_create_master_table.sql):\n",
    "create a sql code file using the bellow code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bff465-5d51-436f-9e22-94c57830f36b",
   "metadata": {},
   "source": [
    "WITH last_order AS (\n",
    "    SELECT\n",
    "        c.customer_unique_id,\n",
    "        MAX(o.order_purchase_timestamp) AS last_purchase_date\n",
    "    FROM customers c\n",
    "    JOIN orders o ON c.customer_id = o.customer_id\n",
    "    GROUP BY c.customer_unique_id\n",
    "),\n",
    "customer_metrics AS (\n",
    "    SELECT\n",
    "        c.customer_unique_id,\n",
    "        COUNT(DISTINCT o.order_id) AS total_orders,\n",
    "        SUM(oi.price + oi.freight_value) AS total_spend,\n",
    "        AVG(oi.price + oi.freight_value) AS avg_spend_per_order,\n",
    "        AVG(r.review_score) AS avg_review_score,\n",
    "        COUNT(DISTINCT p.payment_type) AS num_payment_methods,\n",
    "        -- Replace JULIANDAY with DATEDIFF for customer tenure\n",
    "        DATEDIFF(MAX(o.order_purchase_timestamp), MIN(o.order_purchase_timestamp)) AS customer_tenure_days\n",
    "    FROM customers c\n",
    "    JOIN orders o ON c.customer_id = o.customer_id\n",
    "    JOIN order_items oi ON o.order_id = oi.order_id\n",
    "    JOIN order_reviews r ON o.order_id = r.order_id\n",
    "    JOIN order_payments p ON o.order_id = p.order_id\n",
    "    WHERE o.order_status = 'delivered'\n",
    "    GROUP BY c.customer_unique_id\n",
    ")\n",
    "\n",
    "SELECT\n",
    "    cm.*,\n",
    "    lo.last_purchase_date,\n",
    "    CASE\n",
    "        WHEN DATEDIFF(\n",
    "            (SELECT MAX(order_purchase_timestamp) FROM orders),\n",
    "            lo.last_purchase_date\n",
    "        ) > 180 THEN 1\n",
    "        ELSE 0\n",
    "    END AS churned\n",
    "FROM customer_metrics cm\n",
    "JOIN last_order lo ON cm.customer_unique_id = lo.customer_unique_id;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efd7174-c670-4ff3-b577-e152fbad3a56",
   "metadata": {},
   "source": [
    "### Load into Python:\n",
    "Execute the above query from Python and load the result into a Pandas DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e60fe3-7ced-4c63-ba5c-68fa60656e2a",
   "metadata": {},
   "source": [
    "#### Source Code (Python - 3_load_and_clean.py):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05118797-d19f-4329-8698-9a77215c61c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import mysql.connector\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "engine = create_engine(\"mysql+pymysql://root:Basaram7036%40@localhost/ecommerce_db\")\n",
    "\n",
    "# DB_PATH = 'ecommerce.db'\n",
    "# conn = sqlite3.connect(DB_PATH)\n",
    "\n",
    "# Read the SQL query from the file\n",
    "with open(r\"C:\\Users\\user\\OneDrive\\Documents\\Azgar(DataScience)\\Project\\Chustomer_Churn ML Project\\2_create_master_table.sql\", 'r') as file:\n",
    "    sql_query = file.read()\n",
    "\n",
    "# Execute the query and load data into a DataFrame\n",
    "df = pd.read_sql_query(sql_query, engine)\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17660c7d-23cc-4b75-a002-2ebd522fd13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import mysql.connector\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "engine = create_engine(\"mysql+pymysql://root:Basaram7036%40@localhost/ecommerce_db\")\n",
    "\n",
    "# DB_PATH = 'ecommerce.db'\n",
    "# conn = sqlite3.connect(DB_PATH)\n",
    "\n",
    "# Read the SQL query from the file\n",
    "with open(r\"C:\\Users\\ADMIN\\Desktop\\Customer_Churn_Project\\Chustomer_Churn ML Project\\Chustomer_Churn ML Project\\2_create_master_table.sql\", 'r') as file:\n",
    "    sql_query = file.read()\n",
    "\n",
    "# Execute the query and load data into a DataFrame\n",
    "df = pd.read_sql_query(sql_query, engine)\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c370d9-7250-426f-b5c6-4940682d663d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faae7a18-5d30-45be-a0f3-02e9042d48ae",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960bafc1-0d52-4cf9-80a7-cedb162d8e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Data Cleaning ---\n",
    "# Check for missing values\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0b1541-ec90-400c-97a0-92fc73e2d9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For simplicity, we can fill missing review scores with the median\n",
    "df['avg_review_score'].fillna(df['avg_review_score'].median(), inplace=True)\n",
    "\n",
    "# Display first few rows and info\n",
    "print(df.head())\n",
    "print(df.info())\n",
    "print(df['churned'].value_counts(normalize=True)) # Check for class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5fe089-a189-46fa-a2bc-3d816e07ebd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('analytical_base_table.csv', index=False) # Save the clean data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6af6a8-b15e-4f7a-b361-f1035ae44a5b",
   "metadata": {},
   "source": [
    "# Phase 3: Exploratory Data Analysis (EDA) and Modeling (Python)\n",
    "### Goal: Understand the features, visualize relationships, and build a predictive model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63105c91-64ac-4e70-9260-5dbc403a38b4",
   "metadata": {},
   "source": [
    "#### Source Code (Python - 4_modeling.ipynb): This is best done in a Jupyter Notebook for easy visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9caf741-29b1-4517-b61a-4c4ba96b5a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In a new Jupyter Notebook\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score, roc_curve, confusion_matrix\n",
    "\n",
    "# Load the data created in the previous step\n",
    "df = pd.read_csv('analytical_base_table.csv')\n",
    "\n",
    "# --- EDA ---\n",
    "# Set style for plots\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# Churn rate by number of orders\n",
    "sns.countplot(x='total_orders', hue='churned', data=df[df['total_orders'] < 5])\n",
    "plt.title('Churn Rate by Number of Orders')\n",
    "plt.show()\n",
    "\n",
    "# Churn rate by average review score\n",
    "sns.boxplot(x='churned', y='avg_review_score', data=df)\n",
    "plt.title('Churn vs. Average Review Score')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8396595-77ee-4773-8bc6-d36d9a42d5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Select only numeric columns\n",
    "numeric_df = df.select_dtypes(include=['number'])\n",
    "\n",
    "# Compute correlation matrix\n",
    "corr_matrix = numeric_df.corr()\n",
    "corr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15efb566-c5d1-4aaa-b77b-ec0abff7f29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', square=True)\n",
    "plt.title('Feature Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52de40df-b1fc-495f-af64-6ab837a1b184",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    " # --- Modeling ---\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "# Drop non-predictive columns\n",
    "features = df.drop(columns=['customer_unique_id', 'last_purchase_date', 'churned'])\n",
    "target = df['churned']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42, stratify=target)\n",
    "\n",
    "# Scale numerical features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# --- Model 1: Logistic Regression (as a baseline) ---\n",
    "log_reg = LogisticRegression(random_state=42)\n",
    "log_reg.fit(X_train_scaled, y_train)\n",
    "y_pred_log = log_reg.predict(X_test_scaled)\n",
    "print(\"--- Logistic Regression ---\")\n",
    "print(classification_report(y_test, y_pred_log))\n",
    "print(f\"ROC AUC Score: {roc_auc_score(y_test, log_reg.predict_proba(X_test_scaled)[:, 1]):.4f}\")\n",
    "\n",
    "# # --- Model 2: Random Forest (more powerful) ---\n",
    "# rf = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
    "# rf.fit(X_train, y_train) # Random Forest doesn't strictly require scaling\n",
    "# y_pred_rf = rf.predict(X_test)\n",
    "# print(\"\\n--- Random Forest ---\")\n",
    "# print(classification_report(y_test, y_pred_rf))\n",
    "# print(f\"ROC AUC Score: {roc_auc_score(y_test, rf.predict_proba(X_test)[:, 1]):.4f}\")\n",
    "\n",
    "# # Confusion Matrix for Random Forest\n",
    "# cm = confusion_matrix(y_test, y_pred_rf)\n",
    "# sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "# plt.title('Random Forest Confusion Matrix')\n",
    "# plt.xlabel('Predicted')\n",
    "# plt.ylabel('Actual')\n",
    "# plt.show()\n",
    "\n",
    "# # --- Feature Importance ---\n",
    "# importances = pd.Series(rf.feature_importances_, index=features.columns).sort_values(ascending=False)\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# sns.barplot(x=importances, y=importances.index)\n",
    "# plt.title('Feature Importances from Random Forest')\n",
    "# plt.show()\n",
    "# print(importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a687abfb-a7c8-419c-9ff3-85ca02a5d8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import joblib\n",
    "\n",
    "# joblib.dump(rf, 'C:/Users/user/OneDrive/Desktop/customer charn Dep/Model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c347695c-e909-4d54-b0f4-47a744b9e798",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# --- Improved Random Forest ---\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=250,          # more trees → better learning\n",
    "    max_depth=20,              # deeper trees (was 12)\n",
    "    min_samples_leaf=2,        # allow more splits (was 5)\n",
    "    min_samples_split=5,       # slightly higher split threshold\n",
    "    max_features='sqrt',       # better for high-dimensional data\n",
    "    class_weight='balanced_subsample',  # balance at each bootstrap\n",
    "    random_state=42,\n",
    "    n_jobs=-1                  # use all CPU cores\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"\\n--- Improved Random Forest ---\")\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "print(f\"ROC AUC Score: {roc_auc_score(y_test, rf.predict_proba(X_test)[:, 1]):.4f}\")\n",
    "\n",
    "# --- Save compressed model ---\n",
    "save_folder = r\"C:\\Users\\user\\OneDrive\\Desktop\\customer charn Dep\"\n",
    "os.makedirs(save_folder, exist_ok=True)\n",
    "rf_path = os.path.join(save_folder, \"rf_model_compressed.pkl\")\n",
    "\n",
    "joblib.dump(rf, rf_path, compress=9)  # maximum compression\n",
    "\n",
    "# Check file size\n",
    "file_size = os.path.getsize(rf_path) / (1024 * 1024)\n",
    "print(f\"Compressed Random Forest model size: {file_size:.2f} MB\")\n",
    "print(f\"Model saved at: {rf_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042823d0-8141-4e4c-9be1-c497960becb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Confusion Matrix ---\n",
    "cm = confusion_matrix(y_test, y_pred_rf)\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
    "            xticklabels=['No Churn', 'Churn'],\n",
    "            yticklabels=['No Churn', 'Churn'])\n",
    "plt.title('Random Forest Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n",
    "\n",
    "# --- Feature Importance ---\n",
    "importances = pd.Series(rf.feature_importances_, index=X_train.columns).sort_values(ascending=False)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=importances, y=importances.index, palette=\"viridis\")\n",
    "plt.title('Feature Importances from Random Forest')\n",
    "plt.xlabel('Importance Score')\n",
    "plt.ylabel('Feature')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c312fd-6deb-4b1b-aa69-9c7892a10ebc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
